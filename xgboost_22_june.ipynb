{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d7fb695f-f2e8-459b-8ed1-e56318cfc261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import mplcursors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c2dcfab-b42f-49a2-a5e3-9bbec61bae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "678b64d9-b726-42d3-8b90-13b577036d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=all_data.dropna(subset=['year_of_vehicle_manufacture'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3066ea7-c451-4a9e-92ae-c56a61e7b41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'car_variant', 'year_of_vehicle_manufacture',\n",
       "       'month_of_vehicle_manufacture', 'odometer_reading',\n",
       "       'Odometer_Reading_Present', 'vehicle_fuel_type', 'registered_color',\n",
       "       'vehicle_make', 'vehicle_model', 'accidental_vehicle', 'city',\n",
       "       'car_valuation'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f779ab37-cf8a-4d65-ac40-dd1f00c89ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_to_number(month):\n",
    "    if pd.isna(month):\n",
    "        return 6\n",
    "    try:\n",
    "        # Try to convert directly to an integer\n",
    "        month_number = int(month)\n",
    "        if 1 <= month_number <= 12:\n",
    "            return month_number\n",
    "    except ValueError:\n",
    "        # If month is a string, convert it to lower case and map to month number\n",
    "        month = month.lower()\n",
    "        month_dict = {\n",
    "            'january': 1, 'february': 2, 'march': 3,\n",
    "            'april': 4, 'may': 5, 'june': 6,\n",
    "            'july': 7, 'august': 8, 'september': 9,\n",
    "            'october': 10, 'november': 11, 'december': 12\n",
    "        }\n",
    "        return month_dict.get(month, 6)  # Return 6 if month is not recognized\n",
    "    return 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10601a76-5fdd-482e-a9f9-d68879ed1f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the conversion to the 'month' column\n",
    "all_data['month_of_vehicle_manufacture'] = all_data['month_of_vehicle_manufacture'].apply(month_to_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a884812-eac1-47e3-8241-9d2bdfb6e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate age in months\n",
    "def calculate_age_in_months(row, current_year, current_month):\n",
    "    year, month = row['year_of_vehicle_manufacture'], row['month_of_vehicle_manufacture']\n",
    "    return (current_year - year) * 12 + (current_month - month)\n",
    "\n",
    "# Get current year and month\n",
    "current_year = datetime.now().year\n",
    "current_month = datetime.now().month\n",
    "\n",
    "# Calculate 'ageInMonths' and add it as a new column\n",
    "all_data['months_since_manufactured'] = all_data.apply(calculate_age_in_months, axis=1, current_year=current_year, current_month=current_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "144fd505-f0d9-424b-a6e5-257f23503787",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.drop(columns=['id', 'month_of_vehicle_manufacture', 'year_of_vehicle_manufacture', 'vehicle_make', 'registered_color'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ebe1432-d442-4d8d-a2b5-c29b62307f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['car_variant', 'odometer_reading', 'Odometer_Reading_Present',\n",
       "       'vehicle_fuel_type', 'vehicle_model', 'accidental_vehicle', 'city',\n",
       "       'car_valuation', 'months_since_manufactured'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d47e9ab0-ecf3-42ce-a13a-98992039bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_columns_as_category(columns, data):\n",
    "    for column in columns:\n",
    "        data[column] = data[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4de93588-3ac6-4751-bc84-8e4b7d0b33a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['car_variant', 'vehicle_fuel_type', 'vehicle_model', 'city', 'accidental_vehicle']\n",
    "convert_columns_as_category(categorical_columns, all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d386ba0c-db8a-4701-bfda-946afa36e867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, columns):\n",
    "    feature_importances = model.feature_importances_\n",
    "    print(feature_importances)\n",
    "    \n",
    "    # Create a DataFrame for better visualization\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': columns,\n",
    "        'Importance': feature_importances\n",
    "    })\n",
    "    \n",
    "    # Sort the DataFrame by importance\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # Print the feature importances\n",
    "    print(importance_df)\n",
    "    \n",
    "    # Visualize the feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importances')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1a95105b-c8cf-4e3a-bbac-18ff51e2a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaulate_data(df, model):\n",
    "    train_data = df.sample(frac = 0.8)\n",
    "    test_data = df.drop(train_data.index)\n",
    "\n",
    "    train_inputs = train_data.drop(columns=['car_valuation'], axis=1)\n",
    "    train_output = train_data['car_valuation']\n",
    "\n",
    "    test_inputs = test_data.drop(columns=['car_valuation'], axis=1)\n",
    "    test_output = test_data['car_valuation']\n",
    "\n",
    "    model.fit(train_inputs, train_output)\n",
    "\n",
    "    # mean absolute error on train data\n",
    "    train_prediction = model.predict(train_inputs)\n",
    "    train_error = mean_absolute_error(train_prediction, train_output)\n",
    "\n",
    "    # mean absolute error on test data\n",
    "    test_predicted_output = model.predict(test_inputs)\n",
    "    test_error = mean_absolute_error(test_predicted_output, test_output)\n",
    "\n",
    "    # print(\"test_error= \",test_error,\" train_error= \", train_error)\n",
    "    return test_error, train_error\n",
    "    # plot_feature_importance(model, train_inputs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "605daa65-e1e7-42f2-82c6-c7284e8d4c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors(x_axis, test_errors, train_errors):\n",
    "    # Activate interactive plot in separate window\n",
    "    %matplotlib qt\n",
    "    \n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Plot the data\n",
    "    ax.plot(x_axis, test_errors, label='Test Errors', marker='o')\n",
    "    ax.plot(x_axis, train_errors, label='Train Errors', marker='o')\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('X Axis')\n",
    "    ax.set_ylabel('Error')\n",
    "    ax.set_title('Zoomable Graph with Test and Train Errors')\n",
    "    \n",
    "    # Enable interactive cursor for zooming\n",
    "    mplcursors.cursor(hover=True)\n",
    "    \n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "31a17518-63d4-4baf-99fa-596b733978b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_more_liked= (344, 7)\n",
      "data_less_liked= (158, 7)\n"
     ]
    }
   ],
   "source": [
    "# condition for more interested vehicle, which are non-accidental and has odometer reading\n",
    "condition = (all_data['Odometer_Reading_Present'] != 0) & (all_data['accidental_vehicle'].isna())\n",
    "data_more_liked = all_data[condition]\n",
    "data_less_liked = all_data[~condition]\n",
    "data_more_liked = data_more_liked.drop(columns=['Odometer_Reading_Present', 'accidental_vehicle'], axis=1)\n",
    "data_less_liked = data_less_liked.drop(columns=['Odometer_Reading_Present', 'accidental_vehicle'], axis=1)\n",
    "\n",
    "print('data_more_liked=', data_more_liked.shape)\n",
    "print('data_less_liked=', data_less_liked.shape)\n",
    "\n",
    "max_depths = [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "learning_rates = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "num_estimators = [3, 5, 6, 7, 8, 9, 10, 15, 20, 50, 100]\n",
    "\n",
    "x_axis = []\n",
    "test_errors = []\n",
    "train_errors = []\n",
    "for depth in max_depths:\n",
    "    for learning_rate in learning_rates:\n",
    "        for estimator_count in num_estimators:\n",
    "            model_for_more_liked_data = XGBRegressor(enable_categorical=True, eval_metric='mae', booster='dart', max_depth=depth, learning_rate=learning_rate, n_estimators=estimator_count, random_state=42)\n",
    "            test_error, train_error  = evaulate_data(data_more_liked, model_for_more_liked_data)\n",
    "            if (test_error < 150000) :\n",
    "                x_axis.append(f\"{depth} {learning_rate} {estimator_count}\")\n",
    "                test_errors.append(test_error)\n",
    "                train_errors.append(train_error)\n",
    "\n",
    "plot_errors(x_axis, test_errors, train_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7137680c-b7d3-461f-8641-5f44bef4b283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(173779.677734375), np.float64(53.39930555555556))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_for_more_liked_data = XGBRegressor(enable_categorical=True, eval_metric='mae', booster='dart', max_depth=8, learning_rate=0.525, n_estimators=20, random_state=42)\n",
    "model_for_less_liked_data = XGBRegressor(enable_categorical=True, eval_metric='mae', booster='dart', max_depth=8, learning_rate=0.525, n_estimators=20, random_state=42)\n",
    "\n",
    "evaulate_data(data_more_liked, model_for_more_liked_data)\n",
    "evaulate_data(data_less_liked, model_for_less_liked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "804d430f-b46f-4af9-ac8c-860f7e2ee887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return integer month, from string, default is 6\n",
    "def month_to_number(month):\n",
    "    if pd.isna(month):\n",
    "        return 6\n",
    "    try:\n",
    "        # Try to convert directly to an integer\n",
    "        month_number = int(month)\n",
    "        if 1 <= month_number <= 12:\n",
    "            return month_number\n",
    "    except ValueError:\n",
    "        # If month is a string, convert it to lower case and map to month number\n",
    "        month = month.lower()\n",
    "        month_dict = {\n",
    "            'january': 1, 'february': 2, 'march': 3,\n",
    "            'april': 4, 'may': 5, 'june': 6,\n",
    "            'july': 7, 'august': 8, 'september': 9,\n",
    "            'october': 10, 'november': 11, 'december': 12\n",
    "        }\n",
    "        return month_dict.get(month, 6)  # Return 6 if month is not recognized\n",
    "    return 6\n",
    "\n",
    "# Function to calculate count of months since manufacting data(year, month)\n",
    "def calculate_months_since_manufactured(row, current_year, current_month):\n",
    "    year, month = row['year_of_vehicle_manufacture'], row['month_of_vehicle_manufacture']\n",
    "    return (current_year - year) * 12 + (current_month - month)\n",
    "    \n",
    "# Fuction to mark columns as category type\n",
    "def convert_columns_as_category(columns, data):\n",
    "    for column in columns:\n",
    "        data[column] = data[column].astype('category')\n",
    "\n",
    "# Convert values to uppercase\n",
    "def convert_values_to_uppercase(columns, data):\n",
    "    for column in columns:\n",
    "        data[column] = data[column].str.strip()\n",
    "        data[column] = data[column].str.upper()\n",
    "\n",
    "# Replace text which is matching with a pattern\n",
    "def replace_text(pattern, data, column):\n",
    "    data[column] = data[column].str.replace(pattern, '', regex=True)\n",
    "\n",
    "# Prune data\n",
    "def prune_data(data):\n",
    "    # set month number, considering string, int, nan cases \n",
    "    data['month_of_vehicle_manufacture'] = data['month_of_vehicle_manufacture'].apply(month_to_number)\n",
    "    # add months_since_manufactured column\n",
    "    data['months_since_manufactured'] = data.apply(calculate_months_since_manufactured, axis=1, current_year=datetime.now().year, current_month=datetime.now().month)\n",
    "    # remove redundant features\n",
    "    data = data.drop(columns=['month_of_vehicle_manufacture', 'year_of_vehicle_manufacture', 'registered_color'], axis=1)\n",
    "    # convert values to uppercase\n",
    "    columns_to_be_upper_cased = ['car_variant', 'vehicle_fuel_type', 'vehicle_make', 'vehicle_model', 'vehicle_make', 'city']\n",
    "    convert_values_to_uppercase(columns_to_be_upper_cased, data)\n",
    "    # remove yyyy-yyyy or yyyy from vehicle_model\n",
    "    pattern = r'\\b\\d{4}-\\d{4}\\b|\\b\\d{4}\\b'\n",
    "    replace_text(pattern, data, 'vehicle_model')\n",
    "    # mark columns as category type\n",
    "    categorical_columns = ['car_variant', 'vehicle_fuel_type', 'vehicle_make', 'vehicle_model', 'city']\n",
    "    convert_columns_as_category(categorical_columns, data)\n",
    "    return data\n",
    "\n",
    "# Data segregation basis on likeliness\n",
    "def separate_on_the_basis_of_likeliness(data):\n",
    "    # condition for more interested vehicle is non-accidental and has odometer reading\n",
    "    condition = (data['Odometer_Reading_Present'] != 0) & (data['accidental_vehicle'].isna())\n",
    "    data_more_liked = data[condition]\n",
    "    data_less_liked = data[~condition]\n",
    "    # remove redundant columns\n",
    "    data_more_liked = data_more_liked.drop(columns=['Odometer_Reading_Present', 'accidental_vehicle'], axis=1)\n",
    "    data_less_liked = data_less_liked.drop(columns=['Odometer_Reading_Present', 'accidental_vehicle'], axis=1)\n",
    "    return data_more_liked, data_less_liked\n",
    "\n",
    "# return ids and predicted outcome using model\n",
    "def run_model(data, model):\n",
    "    ids = data['id']\n",
    "    data = data.drop(columns=['id'], axis=1)\n",
    "    predicted_output = model.predict(data)\n",
    "    return ids, predicted_output\n",
    "\n",
    "# predict outcomes and save in file\n",
    "def predict_output_separately_and_save(ids_original_order, data_more_liked, model_for_more_liked_data, data_less_liked, model_for_less_liked_data):\n",
    "    ids_more_liked, prediction_for_more_liked_data = run_model(data_more_liked, model_for_more_liked_data)\n",
    "    ids_less_liked, prediction_for_less_liked_data = run_model(data_less_liked, model_for_less_liked_data)\n",
    "    ids = []\n",
    "    for id_more_liked in ids_more_liked:\n",
    "        ids.append(id_more_liked)\n",
    "    for id_less_liked in ids_less_liked:\n",
    "        ids.append(id_less_liked)\n",
    "    \n",
    "    car_valuations = []\n",
    "    for valuation in prediction_for_more_liked_data:\n",
    "        car_valuations.append(int(valuation))\n",
    "    for valuation in prediction_for_less_liked_data:\n",
    "        car_valuations.append(int(valuation))\n",
    "    print(len(car_valuations))\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'id': ids,\n",
    "        'car_valuation': car_valuations\n",
    "    })\n",
    "    df.to_csv('car_valuation_data.csv', index=False)\n",
    "\n",
    "def predict_output(data, model_for_more_liked_data, model_for_less_liked_data):\n",
    "    ids = data['id']\n",
    "    data = prune_data(data)\n",
    "    data.to_csv('pruned_data.csv', index=False)\n",
    "    data_more_liked, data_less_liked = separate_on_the_basis_of_likeliness(data)\n",
    "    print(data_more_liked.shape, data_less_liked.shape)\n",
    "    predict_output_separately_and_save(ids, data_more_liked, model_for_more_liked_data, data_less_liked, model_for_less_liked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232ce937-e0e3-4b31-8859-5d8435b00a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "50870dff-df7e-4dd7-80ac-27facb0a1297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 8) (22, 8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "feature_names mismatch: ['car_variant', 'odometer_reading', 'vehicle_fuel_type', 'vehicle_model', 'city', 'months_since_manufactured'] ['car_variant', 'odometer_reading', 'vehicle_fuel_type', 'vehicle_make', 'vehicle_model', 'city', 'months_since_manufactured']\ntraining data did not have the following fields: vehicle_make",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[159], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mpredict_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_for_more_liked_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_for_less_liked_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[158], line 108\u001b[0m, in \u001b[0;36mpredict_output\u001b[0;34m(data, model_for_more_liked_data, model_for_less_liked_data)\u001b[0m\n\u001b[1;32m    106\u001b[0m data_more_liked, data_less_liked \u001b[38;5;241m=\u001b[39m separate_on_the_basis_of_likeliness(data)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(data_more_liked\u001b[38;5;241m.\u001b[39mshape, data_less_liked\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 108\u001b[0m \u001b[43mpredict_output_separately_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_more_liked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_for_more_liked_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_less_liked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_for_less_liked_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[158], line 81\u001b[0m, in \u001b[0;36mpredict_output_separately_and_save\u001b[0;34m(ids_original_order, data_more_liked, model_for_more_liked_data, data_less_liked, model_for_less_liked_data)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_output_separately_and_save\u001b[39m(ids_original_order, data_more_liked, model_for_more_liked_data, data_less_liked, model_for_less_liked_data):\n\u001b[0;32m---> 81\u001b[0m     ids_more_liked, prediction_for_more_liked_data \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_more_liked\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_for_more_liked_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     ids_less_liked, prediction_for_less_liked_data \u001b[38;5;241m=\u001b[39m run_model(data_less_liked, model_for_less_liked_data)\n\u001b[1;32m     83\u001b[0m     ids \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[158], line 76\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(data, model)\u001b[0m\n\u001b[1;32m     74\u001b[0m ids \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     75\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m predicted_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids, predicted_output\n",
      "File \u001b[0;32m~/.env/lib/python3.12/site-packages/xgboost/sklearn.py:1186\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[0;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1186\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmargin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1194\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_alike(predts):\n\u001b[1;32m   1195\u001b[0m             \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[0;32m~/.env/lib/python3.12/site-packages/xgboost/core.py:2510\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   2508\u001b[0m     data, fns, _ \u001b[38;5;241m=\u001b[39m _transform_pandas_df(data, enable_categorical)\n\u001b[1;32m   2509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validate_features:\n\u001b[0;32m-> 2510\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_list(data) \u001b[38;5;129;01mor\u001b[39;00m _is_tuple(data):\n\u001b[1;32m   2512\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(data)\n",
      "File \u001b[0;32m~/.env/lib/python3.12/site-packages/xgboost/core.py:3075\u001b[0m, in \u001b[0;36mBooster._validate_features\u001b[0;34m(self, feature_names)\u001b[0m\n\u001b[1;32m   3069\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m my_missing:\n\u001b[1;32m   3070\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   3071\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtraining data did not have the following fields: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3072\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m my_missing)\n\u001b[1;32m   3073\u001b[0m     )\n\u001b[0;32m-> 3075\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names, feature_names))\n",
      "\u001b[0;31mValueError\u001b[0m: feature_names mismatch: ['car_variant', 'odometer_reading', 'vehicle_fuel_type', 'vehicle_model', 'city', 'months_since_manufactured'] ['car_variant', 'odometer_reading', 'vehicle_fuel_type', 'vehicle_make', 'vehicle_model', 'city', 'months_since_manufactured']\ntraining data did not have the following fields: vehicle_make"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('test.csv')\n",
    "predict_output(test_data, model_for_more_liked_data, model_for_less_liked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c0f2b19a-38f7-486a-a1c3-d474d9d765aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_error=  195406.97282608695  train_error=  15087.300227272728\n",
      "test_error=  268460.466796875  train_error=  13984.654761904761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(268460.466796875), np.float64(13984.654761904761))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaulate_data(data_more_liked, model_for_more_liked_data)\n",
    "evaulate_data(data_less_liked, model_for_less_liked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180eb00b-9368-43ae-a87d-f579f10a13f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = all_data.sample(frac = 0.8)\n",
    "test_data = all_data.drop(train_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725a93b5-5754-4ebc-a247-fdf3b0bc39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = train_data.drop(columns=['car_valuation'], axis=1)\n",
    "train_output = train_data['car_valuation']\n",
    "\n",
    "test_inputs = test_data.drop(columns=['car_valuation'], axis=1)\n",
    "test_output = test_data['car_valuation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bbad83-eef2-4cc4-beea-216c5a824430",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78a1c56-2506-443c-b1d4-3b5f1813fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_inputs.shape)\n",
    "print(train_output.shape)\n",
    "print(test_inputs.shape)\n",
    "print(test_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e93b4e7-bc24-4727-b568-cfd5cf3d0e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(enable_categorical=True, booster='dart', max_depth=100, learning_rate=0.1, n_estimators=50)\n",
    "\n",
    "model.fit(train_inputs, train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3a478-80d0-425c-97e4-18fc26e30e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean absolute error on test data\n",
    "test_predicted_output = model.predict(test_inputs)\n",
    "test_error = mean_absolute_error(test_predicted_output, test_output)\n",
    "\n",
    "# mean absolute error on train data\n",
    "train_prediction = model.predict(train_inputs)\n",
    "train_error = mean_absolute_error(train_prediction, train_output)\n",
    "print(\"test_error= \",test_error,\" train_error= \", train_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2be9e7-220e-40b2-a77b-293e76394247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-29 13:03:52.890 Python[36761:177814] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    }
   ],
   "source": [
    "feature_importances = model.feature_importances_\n",
    "print(feature_importances)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': train_inputs.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the feature importances\n",
    "print(importance_df)\n",
    "\n",
    "# Visualize the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f5b29ed0-7989-46c7-9a6b-72960df705a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vikash.yadav/Documents/data_science_hackathon'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de2d49-e7b2-45b7-98e7-928489a7bcdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
